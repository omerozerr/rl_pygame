{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "\n",
    "        q_values = [self.get_q_value(state, action) for action in self.actions]\n",
    "        return self.actions[np.argmax(q_values)]\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        predict = self.get_q_value(state, action)\n",
    "        target = reward + self.gamma * max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "        self.q_table[(state, action)] = predict + self.alpha * (target - predict)\n",
    "\n",
    "# Initialize Q-learning agent\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # up, down, left, right\n",
    "agent = QLearningAgent(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Steps: 29, Total Reward: -18\n",
      "Episode 2/100 - Steps: 29, Total Reward: -18\n",
      "Episode 3/100 - Steps: 29, Total Reward: -18\n",
      "Episode 4/100 - Steps: 50, Total Reward: -39\n",
      "Episode 5/100 - Steps: 40, Total Reward: -29\n",
      "Episode 6/100 - Steps: 23, Total Reward: -12\n",
      "Episode 7/100 - Steps: 21, Total Reward: -10\n",
      "Episode 8/100 - Steps: 34, Total Reward: -23\n",
      "Episode 9/100 - Steps: 39, Total Reward: -28\n",
      "Episode 10/100 - Steps: 28, Total Reward: -17\n",
      "Episode 11/100 - Steps: 35, Total Reward: -24\n",
      "Episode 12/100 - Steps: 38, Total Reward: -27\n",
      "Episode 13/100 - Steps: 33, Total Reward: -22\n",
      "Episode 14/100 - Steps: 22, Total Reward: -11\n",
      "Episode 15/100 - Steps: 33, Total Reward: -22\n",
      "Episode 16/100 - Steps: 56, Total Reward: -45\n",
      "Episode 17/100 - Steps: 32, Total Reward: -21\n",
      "Episode 18/100 - Steps: 32, Total Reward: -21\n",
      "Episode 19/100 - Steps: 37, Total Reward: -26\n",
      "Episode 20/100 - Steps: 20, Total Reward: -9\n",
      "Episode 21/100 - Steps: 24, Total Reward: -13\n",
      "Episode 22/100 - Steps: 22, Total Reward: -11\n",
      "Episode 23/100 - Steps: 24, Total Reward: -13\n",
      "Episode 24/100 - Steps: 37, Total Reward: -26\n",
      "Episode 25/100 - Steps: 33, Total Reward: -22\n",
      "Episode 26/100 - Steps: 31, Total Reward: -20\n",
      "Episode 27/100 - Steps: 20, Total Reward: -9\n",
      "Episode 28/100 - Steps: 26, Total Reward: -15\n",
      "Episode 29/100 - Steps: 21, Total Reward: -10\n",
      "Episode 30/100 - Steps: 18, Total Reward: -7\n",
      "Episode 31/100 - Steps: 34, Total Reward: -23\n",
      "Episode 32/100 - Steps: 43, Total Reward: -32\n",
      "Episode 33/100 - Steps: 32, Total Reward: -21\n",
      "Episode 34/100 - Steps: 34, Total Reward: -23\n",
      "Episode 35/100 - Steps: 26, Total Reward: -15\n",
      "Episode 36/100 - Steps: 35, Total Reward: -24\n",
      "Episode 37/100 - Steps: 25, Total Reward: -14\n",
      "Episode 38/100 - Steps: 26, Total Reward: -15\n",
      "Episode 39/100 - Steps: 21, Total Reward: -10\n",
      "Episode 40/100 - Steps: 27, Total Reward: -16\n",
      "Episode 41/100 - Steps: 31, Total Reward: -20\n",
      "Episode 42/100 - Steps: 27, Total Reward: -16\n",
      "Episode 43/100 - Steps: 26, Total Reward: -15\n",
      "Episode 44/100 - Steps: 39, Total Reward: -28\n",
      "Episode 45/100 - Steps: 20, Total Reward: -9\n",
      "Episode 46/100 - Steps: 27, Total Reward: -16\n",
      "Episode 47/100 - Steps: 37, Total Reward: -26\n",
      "Episode 48/100 - Steps: 34, Total Reward: -23\n",
      "Episode 49/100 - Steps: 18, Total Reward: -7\n",
      "Episode 50/100 - Steps: 38, Total Reward: -27\n",
      "Episode 51/100 - Steps: 18, Total Reward: -7\n",
      "Episode 52/100 - Steps: 37, Total Reward: -26\n",
      "Episode 53/100 - Steps: 39, Total Reward: -28\n",
      "Episode 54/100 - Steps: 34, Total Reward: -23\n",
      "Episode 55/100 - Steps: 26, Total Reward: -15\n",
      "Episode 56/100 - Steps: 20, Total Reward: -9\n",
      "Episode 57/100 - Steps: 18, Total Reward: -7\n",
      "Episode 58/100 - Steps: 25, Total Reward: -14\n",
      "Episode 59/100 - Steps: 22, Total Reward: -11\n",
      "Episode 60/100 - Steps: 28, Total Reward: -17\n",
      "Episode 61/100 - Steps: 28, Total Reward: -17\n",
      "Episode 62/100 - Steps: 31, Total Reward: -20\n",
      "Episode 63/100 - Steps: 21, Total Reward: -10\n",
      "Episode 64/100 - Steps: 30, Total Reward: -19\n",
      "Episode 65/100 - Steps: 31, Total Reward: -20\n",
      "Episode 66/100 - Steps: 23, Total Reward: -12\n",
      "Episode 67/100 - Steps: 34, Total Reward: -23\n",
      "Episode 68/100 - Steps: 31, Total Reward: -20\n",
      "Episode 69/100 - Steps: 23, Total Reward: -12\n",
      "Episode 70/100 - Steps: 22, Total Reward: -11\n",
      "Episode 71/100 - Steps: 35, Total Reward: -24\n",
      "Episode 72/100 - Steps: 38, Total Reward: -27\n",
      "Episode 73/100 - Steps: 21, Total Reward: -10\n",
      "Episode 74/100 - Steps: 32, Total Reward: -21\n",
      "Episode 75/100 - Steps: 22, Total Reward: -11\n",
      "Episode 76/100 - Steps: 41, Total Reward: -30\n",
      "Episode 77/100 - Steps: 21, Total Reward: -10\n",
      "Episode 78/100 - Steps: 20, Total Reward: -9\n",
      "Episode 79/100 - Steps: 18, Total Reward: -7\n",
      "Episode 80/100 - Steps: 26, Total Reward: -15\n",
      "Episode 81/100 - Steps: 30, Total Reward: -19\n",
      "Episode 82/100 - Steps: 24, Total Reward: -13\n",
      "Episode 83/100 - Steps: 32, Total Reward: -21\n",
      "Episode 84/100 - Steps: 21, Total Reward: -10\n",
      "Episode 85/100 - Steps: 22, Total Reward: -11\n",
      "Episode 86/100 - Steps: 26, Total Reward: -15\n",
      "Episode 87/100 - Steps: 27, Total Reward: -16\n",
      "Episode 88/100 - Steps: 20, Total Reward: -9\n",
      "Episode 89/100 - Steps: 27, Total Reward: -16\n",
      "Episode 90/100 - Steps: 22, Total Reward: -11\n",
      "Episode 91/100 - Steps: 28, Total Reward: -17\n",
      "Episode 92/100 - Steps: 25, Total Reward: -14\n",
      "Episode 93/100 - Steps: 19, Total Reward: -8\n",
      "Episode 94/100 - Steps: 20, Total Reward: -9\n",
      "Episode 95/100 - Steps: 40, Total Reward: -29\n",
      "Episode 96/100 - Steps: 23, Total Reward: -12\n",
      "Episode 97/100 - Steps: 24, Total Reward: -13\n",
      "Episode 98/100 - Steps: 30, Total Reward: -19\n",
      "Episode 99/100 - Steps: 27, Total Reward: -16\n",
      "Episode 100/100 - Steps: 21, Total Reward: -10\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "WIDTH, HEIGHT = 500, 500\n",
    "CELL_SIZE = 50\n",
    "GRID_SIZE = WIDTH // CELL_SIZE\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "CYAN = (0, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "\n",
    "# Load images (assuming you have a car.png and flag.png in the current directory)\n",
    "car_img = pygame.transform.scale(pygame.image.load('car.jpg'), (CELL_SIZE, CELL_SIZE))\n",
    "flag_img = pygame.transform.scale(pygame.image.load('flag.jpg'), (CELL_SIZE, CELL_SIZE))\n",
    "\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "\n",
    "def draw_grid():\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(screen, WHITE, (x, 0), (x, HEIGHT))\n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(screen, WHITE, (0, y), (WIDTH, y))\n",
    "\n",
    "def game(num_episodes=100):\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        car_pos = [0, 0]  # Reset starting position at the beginning of each episode\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        running = True\n",
    "        while running:\n",
    "            screen.fill(BLACK)\n",
    "            draw_grid()\n",
    "\n",
    "            # Get agent's current state\n",
    "            current_state = tuple(car_pos)\n",
    "            \n",
    "            # Get agent's action\n",
    "            action = agent.choose_action(current_state)\n",
    "            new_pos = [car_pos[0] + action[0], car_pos[1] + action[1]]\n",
    "\n",
    "            # Check boundaries and update position if it's a valid move\n",
    "            if 0 <= new_pos[0] < GRID_SIZE and 0 <= new_pos[1] < GRID_SIZE:\n",
    "                car_pos = new_pos\n",
    "\n",
    "            # Check for reward\n",
    "            reward = -1  # Default reward for each step\n",
    "            if tuple(car_pos) == (GRID_SIZE - 1, GRID_SIZE - 1):  # If agent reaches the flag\n",
    "                reward = 10\n",
    "                running = False  # End the episode when the agent reaches the flag\n",
    "\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Update Q-values\n",
    "            agent.learn(current_state, action, reward, tuple(car_pos))\n",
    "\n",
    "            # Drawing the car and flag\n",
    "            screen.blit(car_img, (car_pos[0] * CELL_SIZE, car_pos[1] * CELL_SIZE))\n",
    "            screen.blit(flag_img, ((GRID_SIZE - 1) * CELL_SIZE, (GRID_SIZE - 1) * CELL_SIZE))\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            # Refresh the display\n",
    "            pygame.display.flip()\n",
    "\n",
    "            # Cap the frame rate\n",
    "            clock.tick(60)\n",
    "\n",
    "        # Optionally decay epsilon after each episode\n",
    "        agent.epsilon *= 0.9\n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes} - Steps: {steps}, Total Reward: {total_reward}\")\n",
    "\n",
    "game()\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, 0), Q-values: [-5.863546321425413, -5.872396348222972, -5.8705032886611175, -5.864845869830371]\n",
      "State: (1, 0), Q-values: [-5.514880286138465, -5.505565218792635, -5.524011492979922, -5.511677865238788]\n",
      "State: (2, 0), Q-values: [-5.150086297258368, -5.151757277428459, -5.215959798398722, -5.1603560185015755]\n",
      "State: (3, 0), Q-values: [-4.848628825761963, -4.804689768295516, -4.841406702500324, -4.820850386465432]\n",
      "State: (4, 0), Q-values: [-4.473165228376147, -4.450273037523816, -4.523677047577596, -4.462178056292265]\n",
      "State: (5, 0), Q-values: [-4.129632180625154, -4.074695320162007, -4.162175081407496, -4.103099798010827]\n",
      "State: (6, 0), Q-values: [-3.7647460510879984, -3.7993684584388214, -3.76453905643564, -3.7638253101182104]\n",
      "State: (7, 0), Q-values: [-3.4434077942585617, -3.4417705218438046, -3.4744596725576007, -3.4319298434817855]\n",
      "State: (8, 0), Q-values: [-3.0991781120542234, -3.128845930587743, -3.155187317874747, -3.1439474310320987]\n",
      "State: (9, 0), Q-values: [-2.9595973243673996, -2.9465148474634875, -2.9659706322334136, -2.9655230500043057]\n",
      "State: (0, 1), Q-values: [-5.511993029264632, -5.5169783728957205, -5.524767862361893, -5.510966814995938]\n",
      "State: (1, 1), Q-values: [-5.2631019478358665, -5.230361195408349, -5.287648600789291, -5.237045627255783]\n",
      "State: (2, 1), Q-values: [-5.008149770705199, -4.94581992712934, -4.965063145106893, -4.941849069887443]\n",
      "State: (3, 1), Q-values: [-4.674315026604579, -4.623599277238099, -4.675026680209124, -4.640939801257649]\n",
      "State: (4, 1), Q-values: [-4.292370144980057, -4.323439985218195, -4.347739951849744, -4.305661765323431]\n",
      "State: (5, 1), Q-values: [-3.9321987712400417, -3.9666802266077408, -3.9743142791337562, -3.9588530597025677]\n",
      "State: (6, 1), Q-values: [-3.6542119895787533, -3.6033444489458817, -3.6305482875368655, -3.631992548197873]\n",
      "State: (7, 1), Q-values: [-3.3118183344408942, -3.2531873993841693, -3.3026214970510286, -3.278168909568179]\n",
      "State: (8, 1), Q-values: [-2.928044694111439, -2.937287591234719, -2.9173897893003744, -2.9622374719858944]\n",
      "State: (9, 1), Q-values: [-2.6909257319923374, -2.67874071760138, -2.727317124633695, -2.676966303456024]\n",
      "State: (0, 2), Q-values: [-5.172088110057454, -5.156956183092956, -5.150086297258368, -5.1569025777288635]\n",
      "State: (1, 2), Q-values: [-4.937466444906126, -4.950684597788258, -4.9440605426691935, -4.934766795269772]\n",
      "State: (2, 2), Q-values: [-4.736337437284121, -4.682063361361527, -4.698972156845799, -4.68175282895144]\n",
      "State: (3, 2), Q-values: [-4.452409838317785, -4.40680235432764, -4.402024260391373, -4.385655227516266]\n",
      "State: (4, 2), Q-values: [-4.112401916272564, -4.0808713401409715, -4.135787300179727, -4.055027369360175]\n",
      "State: (5, 2), Q-values: [-3.706436301563814, -3.7203365528079133, -3.7025105396570344, -3.7079000945219645]\n",
      "State: (6, 2), Q-values: [-3.335479531503073, -3.2459816009845848, -3.3731855756182605, -3.27663772891064]\n",
      "State: (7, 2), Q-values: [-2.947748191186038, -2.895262537441699, -2.9986859666369137, -2.915150938543037]\n",
      "State: (8, 2), Q-values: [-2.5195591206707273, -2.33859653807186, -2.483565894733202, -2.5164749057649223]\n",
      "State: (9, 2), Q-values: [-2.1996279242583174, -1.8338370782575655, -2.2263219429994483, -2.215287831945849]\n",
      "State: (0, 3), Q-values: [-4.823017788304454, -4.828105477432807, -4.848628825761963, -4.810275910764014]\n",
      "State: (1, 3), Q-values: [-4.62921514388852, -4.6328766224078946, -4.684808064889098, -4.661346236909488]\n",
      "State: (2, 3), Q-values: [-4.443269981641134, -4.423609238631069, -4.455994729450034, -4.408796025400272]\n",
      "State: (3, 3), Q-values: [-4.143596839142853, -4.113050019602101, -4.186057797607893, -4.112195573056215]\n",
      "State: (4, 3), Q-values: [-3.8295355197320546, -3.7547361127152765, -3.7740996197860253, -3.765149987130105]\n",
      "State: (5, 3), Q-values: [-3.338275544987962, -3.1651925911187755, -3.389804013095137, -3.306744123040037]\n",
      "State: (6, 3), Q-values: [-2.919877956042753, -2.2768541186385742, -2.850166362031688, -2.8485785769446417]\n",
      "State: (7, 3), Q-values: [-2.5093743524569483, -2.2194194845931356, -2.4679320109563667, -2.4296236459135536]\n",
      "State: (8, 3), Q-values: [-2.0412876237882687, -1.001731131172903, -2.055533277271021, -2.0138465632095586]\n",
      "State: (9, 3), Q-values: [-1.7652264914798306, -0.1359337796132768, -1.8068077754646579, -1.7383137616441333]\n",
      "State: (0, 4), Q-values: [-4.532037495943664, -4.467343171108269, -4.471595316024819, -4.49404276681387]\n",
      "State: (1, 4), Q-values: [-4.3235295485628695, -4.3356928830340395, -4.359550950193465, -4.330071779193103]\n",
      "State: (2, 4), Q-values: [-4.1622219360601065, -4.118567634107767, -4.12795061856604, -4.095412074271844]\n",
      "State: (3, 4), Q-values: [-3.8243347187036236, -3.766976089914395, -3.844545603679206, -3.773719380059501]\n",
      "State: (4, 4), Q-values: [-3.3963663489498104, -3.3467318837680207, -3.376549152144624, -3.264232819534805]\n",
      "State: (5, 4), Q-values: [-2.904251776702763, -2.8646046897466166, -2.8991319518571865, -2.0135961727386222]\n",
      "State: (6, 4), Q-values: [-2.4380005414130257, -0.6303530231103831, -2.4019116250393244, -2.396473062716396]\n",
      "State: (7, 4), Q-values: [-1.9880664222186262, -0.6617876528496711, -2.01969478056636, -1.9469097017439976]\n",
      "State: (8, 4), Q-values: [-1.5987857034768362, 1.0410182341613172, -1.6139829853124763, -1.5337873033490643]\n",
      "State: (9, 4), Q-values: [-1.3355203720522788, 2.2650614471335233, -1.2583812767107572, -1.3125418723102174]\n",
      "State: (0, 5), Q-values: [-4.125391916689492, -4.149560885403147, -4.129632180625154, -4.141647220738349]\n",
      "State: (1, 5), Q-values: [-4.029431233408105, -3.99197096628829, -4.0110389697917626, -4.022519395880274]\n",
      "State: (2, 5), Q-values: [-3.8010396382277163, -3.7768971065737755, -3.776693270060325, -3.7880866057920475]\n",
      "State: (3, 5), Q-values: [-3.4055590919433554, -3.3761672848966344, -3.451908370300134, -3.3621109593040277]\n",
      "State: (4, 5), Q-values: [-2.974268908594952, -2.9154747087779187, -2.9117670913577745, -2.4890802995883097]\n",
      "State: (5, 5), Q-values: [-2.4715364114433043, -2.439250490392741, -2.52258946598423, -1.0564901545624117]\n",
      "State: (6, 5), Q-values: [-2.0073888877345527, -1.9652882117831196, -2.058785249555124, 0.5922553222172522]\n",
      "State: (7, 5), Q-values: [-1.5303002813279436, -1.5146755703114434, -1.5490868069994703, 1.802133824571043]\n",
      "State: (8, 5), Q-values: [-1.1628204731186578, -1.0682474220372924, -1.129685229968105, 3.120850091147726]\n",
      "State: (9, 5), Q-values: [-0.8374025790940561, 4.579812931523662, -0.7963966139207154, -0.8640649262425764]\n",
      "State: (0, 6), Q-values: [-3.8518600755397827, -3.8145933004665147, -3.827098590577118, -3.8105025218827033]\n",
      "State: (1, 6), Q-values: [-3.6783188889170475, -3.6799653472129927, -3.6544948749243167, -3.682710120569526]\n",
      "State: (2, 6), Q-values: [-3.4100467961203385, -3.4107498073521105, -3.4344735159177806, -3.4360454400510765]\n",
      "State: (3, 6), Q-values: [-3.0795099681285025, -2.96385282669279, -3.0477805091325267, -3.0042036700949435]\n",
      "State: (4, 6), Q-values: [-2.5386191498694957, -2.3831493353702324, -2.592883749884648, -2.5182844562738063]\n",
      "State: (5, 6), Q-values: [-2.1091143422347254, -1.7732718225397823, -2.048993270311645, -2.0235176682048097]\n",
      "State: (6, 6), Q-values: [-1.6486741776394875, -1.337228764980906, -1.5671621765022345, -1.5641032213565405]\n",
      "State: (7, 6), Q-values: [-1.1739691350944523, -1.0640409461476172, -1.1425599350566762, -0.8570102810685833]\n",
      "State: (8, 6), Q-values: [-0.7497727648010287, -0.6346154351283433, -0.7417355098693721, 1.302135053269886]\n",
      "State: (9, 6), Q-values: [-0.40480512319000006, 6.199968341627893, -0.39337651, -0.3940399]\n",
      "State: (0, 7), Q-values: [-3.5282365153413973, -3.5288537871966774, -3.508973716315976, -3.509161808475741]\n",
      "State: (1, 7), Q-values: [-3.379528797074639, -3.357622968858329, -3.3898682507011286, -3.3705525714822313]\n",
      "State: (2, 7), Q-values: [-3.011163171236184, -3.0346949846970848, -3.0560746701380395, -2.9666625917375335]\n",
      "State: (3, 7), Q-values: [-2.599447707418744, -2.5451798008859554, -2.5481176871381033, -1.9002690845344965]\n",
      "State: (4, 7), Q-values: [-2.06440318810119, -2.058199849540357, -2.146342590499096, -0.38343849648836464]\n",
      "State: (5, 7), Q-values: [-1.6261285001098575, -1.5431028012171488, -1.574920435959939, 1.2775218132284485]\n",
      "State: (6, 7), Q-values: [-1.1413262755095535, 2.9280104348515406, -1.0689218127669755, -1.0531964196383592]\n",
      "State: (7, 7), Q-values: [-0.6127095143136282, -0.055367136369584624, -0.6436405264062341, -0.6214435656555828]\n",
      "State: (8, 7), Q-values: [-0.3138499, -0.307461511, -0.315365491, 0.5092896374109206]\n",
      "State: (9, 7), Q-values: [-0.199, 7.999996392003585, -0.10900000000000001, -0.1]\n",
      "State: (0, 8), Q-values: [-3.282264672284195, -3.289357944098813, -3.2427095093971667, -3.2788987007830896]\n",
      "State: (1, 8), Q-values: [-3.136304096761821, -3.1151102113073166, -3.106860200617828, -3.114035035197851]\n",
      "State: (2, 8), Q-values: [-2.7997991476388684, -2.7214340896817855, -2.7422005105355725, -2.72031764200029]\n",
      "State: (3, 8), Q-values: [-2.1812107014327644, -2.1847083616663143, -2.2586353957787617, -1.8763188068980028]\n",
      "State: (4, 8), Q-values: [-1.6823603973608998, -1.7020145798290474, -1.7119524463542655, -0.26337789421575836]\n",
      "State: (5, 8), Q-values: [-1.20989249082975, -1.2070316090420001, -1.2690524906315945, 2.0981283346514954]\n",
      "State: (6, 8), Q-values: [-0.7036085976697001, -0.6902078321413496, -0.7558680461270793, 4.542512569217634]\n",
      "State: (7, 8), Q-values: [-0.29701, -0.2962, -0.32192649100000004, 6.191724885137184]\n",
      "State: (8, 8), Q-values: [-0.199, 7.999033150342317, -0.1, -0.1]\n",
      "State: (9, 8), Q-values: [-0.1, 9.999999794533233, -0.08268310000000001, 0.0]\n",
      "State: (0, 9), Q-values: [-3.1561841702123616, -3.10550914130922, -3.1049475453005146, -3.1477283122860893]\n",
      "State: (1, 9), Q-values: [-3.0220171192330882, -2.9655230500043057, -2.9773732024063286, -2.9408845356421986]\n",
      "State: (2, 9), Q-values: [-2.5462964216001804, -2.5282790566840365, -2.6054031349640643, -2.5299099012222634]\n",
      "State: (3, 9), Q-values: [-1.9618139102781271, -1.9766363125959923, -1.9934739203823435, -1.6989661152290119]\n",
      "State: (4, 9), Q-values: [-1.432524765527564, -1.4776035767363227, -1.4313751124297396, -0.1455982730266558]\n",
      "State: (5, 9), Q-values: [-0.976610990323493, -0.9561792499119552, -0.9531371058218633, 2.1929933250854154]\n",
      "State: (6, 9), Q-values: [-0.49214247830200003, -0.490099501, -0.5510157095647091, 4.985906536251575]\n",
      "State: (7, 9), Q-values: [-0.199, -0.2881, -0.19981000000000002, 7.677650010044225]\n",
      "State: (8, 9), Q-values: [-0.1, -0.1, -0.10900000000000001, 9.99999087965544]\n",
      "State: (9, 9), Q-values: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for y in range(GRID_SIZE):\n",
    "        for x in range(GRID_SIZE):\n",
    "            state = (x, y)\n",
    "            q_values = [agent.get_q_value(state, action) for action in agent.actions]\n",
    "            print(f\"State: {state}, Q-values: {q_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
